@INPROCEEDINGS{8250706, 
author={P. Seth and N. Rane and A. Wagh and A. Katade and S. Sahu and N. Malhotra}, 
booktitle={2017 International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
title={Uberisation of mobile automation testing}, 
year={2017}, 
volume={}, 
number={}, 
pages={181-183}, 
abstract={Mobile phones and mobile applications have now become an essential part of everyday life. To make Mobile applications more reliable and error free, mobile application testing is important. Currently only a few techniques exist for creating automate tests of mobile applications and their functionality is very limited. In this paper, we introduce the new way of implementing a mobile test automation platform which performs mobile test automation from mobile devices itself. The main aim of automating the testing process is to develop a high quality and optimized applications to deliver efficient results to the customer.}, 
keywords={mobile computing;mobile handsets;program testing;Mobile phones;mobile application testing;mobile automation testing;mobile devices;mobile test automation platform;Androids;Automation;Mobile applications;Mobile communication;Mobile handsets;Testing;Tools;Device automation;Mobile app testing;Software Engineering;Software quality;Test Automation;Wireless testing}, 
doi={10.1109/ICCONS.2017.8250706}, 
ISSN={}, 
month={June},}
@INPROCEEDINGS{1625700, 
author={Bum Hyun Lim and Jin Ryong Kim and Kwang Hyun Shim}, 
booktitle={2006 8th International Conference Advanced Communication Technology}, 
title={A load testing architecture for networked virtual environment}, 
year={2006}, 
volume={1}, 
number={}, 
pages={5 pp.-848}, 
abstract={In this work, we develop a load testing architecture for networked virtual environment to reduce the testing time and ensure the stability of the server for distributed applications. It explicitly secures the stability of the server for networked virtual environment and at the same time, it elaborately generates actual loads for testing the performance of the server. Our agent based load testing architecture provides variety of interactions of virtual entities in the virtual worlds to perform realistic simulations. Simulation results show that our proposed architecture ensures the stability and capacity of the servers}, 
keywords={client-server systems;resource allocation;distributed applications;load testing architecture;networked virtual environment;server stability;virtual client;Analytical models;Databases;Discrete event simulation;Environmental management;Large-scale systems;Libraries;Network servers;Protocols;Testing;Virtual environment;Load test;beta test;game simulator;networked virtual environment;stress test;virtual client}, 
doi={10.1109/ICACT.2006.206095}, 
ISSN={}, 
month={Feb},}
@INPROCEEDINGS{5552290, 
author={D. Hao and Y. Chen and F. Tang and F. Qi}, 
booktitle={2010 IEEE International Conference on Software Engineering and Service Sciences}, 
title={Distributed agent-based performance testing framework on Web Services}, 
year={2010}, 
volume={}, 
number={}, 
pages={90-94}, 
abstract={Web Services are applied widely in the field of information technology, and performance testing for Web Services applications has become an important problem. This paper introduces the method of performance testing, and proposes a framework of agent-based performance testing on Web Services, which includes TestFlow Generator, Scenario Creator, Test Manager, Load Generation Agent and Test Analyzer. And the implementation of kernel modules in the framework is introduced especially. To realize the load allocation to distributed Load Generation Agents from Test Manager, a queue-based allocation strategy is given.}, 
keywords={Load modeling;Monitoring;Resource management;Schedules;Testing;Web services;Web Services;allocation strategy;load generation;performance testing}, 
doi={10.1109/ICSESS.2010.5552290}, 
ISSN={2327-0586}, 
month={July},}
@ARTICLE{1377198, 
author={A. Avritzer and E. J. Weyuker}, 
journal={IEEE Transactions on Software Engineering}, 
title={The role of modeling in the performance testing of e-commerce applications}, 
year={2004}, 
volume={30}, 
number={12}, 
pages={1072-1083}, 
abstract={An e-commerce scalability case study is presented in which both traditional performance testing and performance modeling were used to help tune the application for high performance. This involved the creation of a system simulation model as well as the development of an approach for test case generation and execution. We describe our experience using a simulation model to help diagnose production system problems, and discuss ways that the effectiveness of performance testing efforts was improved by its use.}, 
keywords={Java;electronic commerce;program testing;resource allocation;software performance evaluation;e-commerce;production system diagnosis;software performance modeling;software performance testing;test case generation;workload characterization;Aerospace testing;Computer architecture;Databases;Helium;Java;Monitoring;Production systems;Scalability;Software testing;System testing}, 
doi={10.1109/TSE.2004.107}, 
ISSN={0098-5589}, 
month={Dec},}
@INPROCEEDINGS{6570626, 
author={A. J. Maâlej and M. Hamza and M. Krichen}, 
booktitle={2013 Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises}, 
title={WSCLT: A Tool for WS-BPEL Compositions Load Testing}, 
year={2013}, 
volume={}, 
number={}, 
pages={272-277}, 
abstract={This paper addresses the load testing of WS-BPEL compositions. For that, we developed WSCLT tool, which takes as input a specification of the composition under test, expressed as a Timed Automaton, and considers various parameters such as the number of requests to handle simultaneously. Our WSCLT tool injects this load in the application and monitors the sequence of requests, invocations and responses between the components. This log is then analyzed by the tool to separate the actions corresponding to each instance and to check that they follow legitimate paths. A global report is then issued regarding all concurrent instances. We illustrate how to use our prototype tool by means of a case study.}, 
keywords={Web services;automata theory;program testing;WS-BPEL compositions;WSCLT;load testing;timed automaton;Atmospheric modeling;Automata;Delays;Load modeling;Queueing analysis;Testing;Web services;Timed Automaton;WS-BPEL compositions;load testing;log analysis}, 
doi={10.1109/WETICE.2013.71}, 
ISSN={1524-4547}, 
month={June},}
@INPROCEEDINGS{6317675, 
author={C. Williamette and E. Hansen}, 
booktitle={2012 38th IEEE Photovoltaic Specialists Conference}, 
title={Development of electrical performance testing standards for the acceptance of solar photovoltaic projects based on field experience and observation}, 
year={2012}, 
volume={}, 
number={}, 
pages={000554-000559}, 
abstract={As-built performance requirements are becoming more common in Interconnection Applications (IAs) and Power Purchase Agreements (PPAs). Often overlooked as, “just the last step in the commissioning process,” it is important to understand the scope of the testing requirements before committing to the agreement. The worst case scenario is when your project has design flaws that prevent it from meeting requirements. By the time the system is discovered to be failing, it can be too late and too costly to fix. Understanding standards for performance testing can help guide project design to ensure better success meeting those requirements later on.}, 
keywords={commissioning;interconnections;photovoltaic power systems;solar power stations;standards;PPA;commissioning process;electrical performance testing standards;field experience;interconnection applications;power purchase agreements;solar PV systems;solar photovoltaic projects;Indexes;Inverters;Irrigation;Monitoring;Soil;Wiring;Current-voltage characteristics;Performance Analysis;Soil measurements;Solar energy;System analysis and design;Thermal analysis}, 
doi={10.1109/PVSC.2012.6317675}, 
ISSN={0160-8371}, 
month={June},}
@INPROCEEDINGS{5405721, 
author={G. h. Kim and H. c. Moon and G. P. Song and S. K. Shin}, 
booktitle={Proceedings of the 4th International Conference on Ubiquitous Information Technologies Applications}, 
title={Software Performance Testing Scheme Using Virtualization Technology}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={In this paper, we propose software performance testing scheme using virtualization technology. Generally, people have used software performance testing tool such as load runner and silk performer. However, people should perform software performance testing manually in case of the situation that people cannot use testing tool. It needs a lot of resources such as human resource and computing resource. Therefore, we propose software performance testing scheme using virtualization technology to reduce resource consumption. Proposed scheme can reduce computer resource consumption because virtualization technology can make a number of virtual computers with a small number of physical computers. Also proposed scheme can reduce human resource consumption because, in proposed scheme, management computer can control keyboard and mouse of virtual computers automatically. Simulation result shows that proposed scheme has possibility to be used for software performance testing.}, 
keywords={software performance evaluation;virtual machines;computer management;computing resource;human resource;software performance testing;virtual computers;virtualization technology;Automatic control;Computational modeling;Human resource management;Keyboards;Mice;Performance evaluation;Physics computing;Resource virtualization;Software performance;Software testing}, 
doi={10.1109/ICUT.2009.5405721}, 
ISSN={1976-0035}, 
month={Dec},}
@ARTICLE{4015510, 
author={D. Krishnamurthy and J. A. Rolia and S. Majumdar}, 
journal={IEEE Transactions on Software Engineering}, 
title={A Synthetic Workload Generation Technique for Stress Testing Session-Based Systems}, 
year={2006}, 
volume={32}, 
number={11}, 
pages={868-882}, 
abstract={Enterprise applications are often business critical but lack effective synthetic workload generation techniques to evaluate performance. These workloads are characterized by sessions of interdependent requests that often cause and exploit dynamically generated responses. Interrequest dependencies must be reflected in synthetic workloads for these systems to exercise application functions correctly. This poses significant challenges for automating the construction of representative synthetic workloads and manipulating workload characteristics for sensitivity analyses. This paper presents a technique to overcome these problems. Given request logs for a system under study, the technique automatically creates a synthetic workload that has specified characteristics and maintains the correct interrequest dependencies. The technique is demonstrated through a case study involving a TPC-W e-commerce system. Results show that incorrect performance results can be obtained by neglecting interrequest dependencies, thereby highlighting the value of our technique. The study also exploits our technique to investigate the impact of several workload characteristics on system performance. Results establish that high variability in the distributions of session length, session idle times, and request service times can cause increased contention among sessions, leading to poor system responsiveness. To the best of our knowledge, these are the first results of this kind for a session-based system. We believe our technique is of value for studies where fine control over workload is essential}, 
keywords={electronic commerce;performance evaluation;program testing;TPC-W e-commerce system;e-commerce system;enterprise application;performance evaluation;sensitivity analyses;stress testing session-based system;synthetic workload generation technique;Application software;Character generation;Computer Society;Delay;Occupational stress;Sensitivity analysis;Stress control;System performance;System testing;Web server;Internet applications;Performance of systems;Web servers.;electronic commerce;measurement techniques;modeling techniques;software engineering;testing tools}, 
doi={10.1109/TSE.2006.106}, 
ISSN={0098-5589}, 
month={Nov},}
@INPROCEEDINGS{7397245, 
author={A. Ali and N. Badr}, 
booktitle={2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
title={Performance testing as a service for web applications}, 
year={2015}, 
volume={}, 
number={}, 
pages={356-361}, 
abstract={Software testing is a vital activity that is undertaken during software engineering life cycle to ensure software quality and reliability. Performance testing is a type of software testing that is done to shows how web application behaves under a certain workload. Cloud computing as an emerging technology can be used in the field of software engineering to provide cloud testing in order to overcome all deficiencies of conventional testing by leveraging cloud computing resources. As a result, testing-as-a-service (TaaS) is introduced as a service model that performs all testing activities in fully automated manner, on demand with a pay-for use basis. Moreover, TaaS increases testing efficiency and reduces time and cost required for testing. In this paper, performance TaaS framework for web applications is introduced which provides all performance testing activities including automatic test case generation and test execution. In addition, the proposed framework addresses many issues as: maximize resource utilization and continuous monitoring to ensure system reliability.}, 
keywords={Web services;program testing;software performance evaluation;software quality;Web applications;automatic test case generation;cloud computing resources;cloud testing;continuous monitoring;performance testing;software engineering life cycle;software quality;software reliability;software testing;system reliability;testing-as-a-service;Fault tolerance;Fault tolerant systems;Software;Testing;Virtualization;Cloud Computing;JMeter;Performance Testing;TaaS;web Application Testing}, 
doi={10.1109/IntelCIS.2015.7397245}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{5729579, 
author={R. W. Y. Habash and V. Groza and Y. Yang and C. Blouin and P. Guillemette}, 
booktitle={2011 Sixth IEEE International Symposium on Electronic Design, Test and Application}, 
title={Performance Testing and Control of a Small Wind Energy Converter}, 
year={2011}, 
volume={}, 
number={}, 
pages={263-268}, 
abstract={Responding to more demand in coming years, the task of the small wind energy industry requires progress on several fronts-from public policy initiatives, to technology development, to market growth. Enhanced technologies such as contra-rotating blades, transmission systems, lubrication, airfoils, generators, and power electronics will lower cost and increase energy production. This paper mainly considers two key technological points of a small wind energy converter (SWEC) namely, the performance of the rotor system and induction generator. Small-scale prototypes have been built to experimentally verify the performance of the SWEC. Wind tunnel tests of the power output, power coefficient, and turbine speed were carried out to ascertain the aerodynamic power conversion and the operation capability at lower wind speeds. The results demonstrated a significant increase in performance compared to a single-rotor system of the same type. Another aspect of development and test is to present a comparative performance evaluation between a standard induction generator and an efficient but with modified design (TRIAS Generator) as a realistic solution of clean power for grid-connected SWECs. The paper also discusses issues related to control and monitoring of SWEC.}, 
keywords={aerodynamics;asynchronous generators;power convertors;power generation control;power grids;power markets;rotors;wind power plants;wind tunnels;wind turbines;aerodynamic power conversion;grid connected SWEC;induction generator;market growth;performance testing;public policy;rotor system;small scale prototype;small wind energy converter control;technology development;wind energy industry;wind tunnel;Blades;Generators;Induction motors;Rotors;Wind energy;Wind speed;Wind turbines;Small wind generator;contra-rotating system;induction generator}, 
doi={10.1109/DELTA.2011.55}, 
ISSN={}, 
month={Jan},}
@ARTICLE{536458, 
author={D. Grossman and M. C. McCabe and C. Staton and B. Bailey and O. Frieder and D. C. Roberts}, 
journal={IEEE Software}, 
title={Performance testing a large finance application}, 
year={1996}, 
volume={13}, 
number={5}, 
pages={50-54}, 
abstract={The case study presented in the paper shows how a simple prototype can be used to verify, before production, that a system will perform at an acceptable level under realistic conditions. The study involves the first implementation of American Management System's Federal Financial System (FFS), a financial accounting application, in a customer information control system (CICS) DB2 environment running on a large IBM mainframe}, 
keywords={accounts data processing;financial data processing;program testing;program verification;relational databases;software performance evaluation;American Management System;DB2 environment;Federal Financial System;IBM mainframe;case study;customer information control system;financial accounting application;large finance application;performance testing;program verification;relational database;simple prototype;Control systems;Database systems;Delay;Environmental management;Finance;Financial management;Information technology;Operating systems;Performance evaluation;Production systems;Prototypes;Stress;System testing;Technology management;Testing}, 
doi={10.1109/52.536458}, 
ISSN={0740-7459}, 
month={Sep},}
@INPROCEEDINGS{6131250, 
author={S. Duttagupta and M. Nambiar}, 
booktitle={2011 UKSim 5th European Symposium on Computer Modeling and Simulation}, 
title={Performance Extrapolation for Load Testing Results of Mixture of Applications}, 
year={2011}, 
volume={}, 
number={}, 
pages={424-429}, 
abstract={Load testing of IT applications faces the challenge of providing high quality test results that would represent the performance in production like scenarios, without incurring high cost of commercial load testing tools. It would help IT projects to be able to test with a small number of users and extrapolate to scenarios with much larger number of users. Such an extrapolation strategy when applied to mixture of application workloads running on a shared server environment must take into consideration application characteristics (CPU/IO intensive, memory bound) as well the server capabilities. The goal is to predict the performance of mixture workload, the maximum throughput offered by the application mix and the maximum number of users supported by the system before the throughput starts degrading. In this paper, we propose an extrapolation strategy that analyses a system workload mix based on its service demand on various resources and extrapolates its performance using simple empirical modeling techniques. Moreover, its ability to extrapolate throughput of an application mixture even if there is a change in the mixture, can help in capacity planning of the system.}, 
keywords={extrapolation;program testing;IT application;application mixture;empirical modeling technique;extrapolation strategy;information technology;load testing;Extrapolation;Load modeling;Production;Servers;Telecommunications;Testing;Throughput;Extrapolation;S-curve;load Testing;mixture of applications;multi-classes of job}, 
doi={10.1109/EMS.2011.56}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{6449887, 
author={M. Singh and R. Singh}, 
booktitle={2012 2nd IEEE International Conference on Parallel, Distributed and Grid Computing}, 
title={Load Testing of web frameworks}, 
year={2012}, 
volume={}, 
number={}, 
pages={592-596}, 
abstract={This document deals with a comparative analysis on the web frame works namely Spring3.0 MVC, Struts 2.0, JSF 1.2x and Wickets. A detailed study is given on the behavior of the frameworks when they are utilized in the front end and at the backend JPA is used to make communication with the database. The Database utilized is Oracle 10g. Load Testing of all the applications is done using J-meter.}, 
keywords={Internet;database management systems;online front-ends;program testing;J-meter;JSF 1.2x;Oracle 10g;Spring3.0 MVC;Struts 2.0;Web frameworks;Wickets;backend JPA;database;front end;load testing;Bandwidth;Color;Information filters;Process control;Springs;Throughput;JSF 1.2x;Spring3.0 MVC;Struts 2.0;Wickets and JPA}, 
doi={10.1109/PDGC.2012.6449887}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6927571, 
author={A. Freitas and R. Vieira}, 
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)}, 
title={An Ontology for Guiding Performance Testing}, 
year={2014}, 
volume={1}, 
number={}, 
pages={400-407}, 
abstract={Software test is a technique to obtain information about software systems quality. Performance test is a type of software test that aims at evaluating software performance at a given load scenario, but it requires specialized knowledge about tools, activities and metrics of the domain. Since ontology is a promising knowledge representation technique, this paper presents a literature review to identify trends and compare researches of ontologies in the fields of software testing and software performance. Also, to investigate this issue from a practical perspective, it was developed an ontology for representing the core knowledge of performance testing. This paper presents the ontology and compare it with related ones. Then, semantic technologies are explored to demonstrate the practical feasibility of developing ontology-based applications for assisting testers with performance test planning and management.}, 
keywords={ontologies (artificial intelligence);program testing;software management;software performance evaluation;software quality;knowledge representation technique;ontology;performance test management;performance test planning;performance testing guidance;semantic technologies;software performance evaluation;software systems quality;software testing;Measurement;OWL;Ontologies;Software performance;Software testing}, 
doi={10.1109/WI-IAT.2014.62}, 
ISSN={}, 
month={Aug},}
@INPROCEEDINGS{8203630, 
author={H. Khandelwal and P. Mankodi and R. Prajapati}, 
booktitle={2017 International conference of Electronics, Communication and Aerospace Technology (ICECA)}, 
title={Enhancement of automation testing system using Yocto project}, 
year={2017}, 
volume={1}, 
number={}, 
pages={697-700}, 
abstract={Nowadays, in industries, Testing has become one of the important tasks. Testing is necessary for an effective performance of product and software applications. When companies having a mass production of hardware boards, it is necessary to do testing of each and every module of hardware like SPI, UART, GPIO, PWM, ADC, USB, Ethernet. If we do testing manually then it will significantly take more time, which we can be reduced by automation testing. But the solution is not an automation testing because automation testing also requires the same amount of system, for example for 300 boards 300 system is required. The only difference in manual and automation testing is that in automation testing it will require less human effort. Now we are focusing more on developing a solution in which many tests can be done on one single system i.e. for 300 boards only one system is required for testing hence this problem can be solved by Yocto Project. Yocto project have build the tool named Bitbake which is written in Python language, which works on multithreading and scheduling so that simultaneously you can test more boards on a single system. In this paper we did automation testing of GPIO pins using Yocto project.}, 
keywords={automatic test software;Bitbake tool;GPIO pins;Python language;Yocto Project;automation testing system;multithreading;scheduling;Automation;Hardware;Licenses;Pins;Software;Testing;Tools;Automation;Bitbake;Open embedded;Yocto project}, 
doi={10.1109/ICECA.2017.8203630}, 
ISSN={}, 
month={April},}
@INPROCEEDINGS{883774, 
author={B. M. Subraya and S. V. Subrahmanya}, 
booktitle={Proceedings First Asia-Pacific Conference on Quality Software}, 
title={Object driven performance testing of Web applications}, 
year={2000}, 
volume={}, 
number={}, 
pages={17-26}, 
abstract={Performance of many Web sites depends on the load on the site at peak time under varying conditions. Performance testing is normally conducted in reasonably simulated environment with the help of performance testing tools. However, performance of a Web site depends on various parameters and each parameter must be tested under varying stress levels. It is not possible to draw a common denominator for performance parameters to test the Web site due to complexity of Web sites. Different parts of the Web site must be tested with different parameters under varying condition and stress level. In such circumstances, it is necessary to decompose the Web site into many components, which represents the behavior of various business components. These business components are mapped to various objects that truly represent the behavior and structure of the part of the web site. These objects are subjected to performance testing with different parameters and stress levels. This paper addresses the new testing process, which uses the concept of decomposing the behavior of the Web site into testable components, which are mapped onto testable objects. These testable objects are subjected to performance testing under varied performance parameters and stress levels}, 
keywords={computational complexity;information resources;program testing;programming environments;software performance evaluation;Web applications;Web sites;complexity;object driven performance testing;performance parameters;simulated environment;Acoustic testing;Application software;Cities and towns;Consumer electronics;Electronic commerce;Life testing;Software testing;Stress;System testing;Time to market}, 
doi={10.1109/APAQ.2000.883774}, 
ISSN={}, 
month={},}
@INPROCEEDINGS{6354629, 
author={N. Baltas and T. Field}, 
booktitle={2012 Ninth International Conference on Quantitative Evaluation of Systems}, 
title={Continuous Performance Testing in Virtual Time}, 
year={2012}, 
volume={}, 
number={}, 
pages={13-22}, 
abstract={In this paper we show how program code and performance models can be made to cooperate seamlessly to support continuous software performance testing throughout the development lifecycle. We achieve this by extending our existing VEX tool for executing programs in virtual time so that events that occur during normal execution and those that occur during the simulation of a performance model can be scheduled on a single global virtual time line. The execution time of an incomplete component of an application is thus estimated by a performance model, whilst that of existing code is measured by instrumentation that is added dynamically at program load time. A key challenge is to be able to map some or all of the resources in a performance model to the real resources of the host platform on which the application is running. We outline a continuous performance engineering methodology that exploits our unified framework and illustrate the principles involved byway of a simple Java application development case study.}, 
keywords={Java;program testing;software performance evaluation;software tools;Java application development;VEX tool;continuous performance engineering methodology;continuous software performance testing;development lifecycle;performance model;program code;program execution;program load time;virtual time;Computational modeling;Instruction sets;Java;Predictive models;Real-time systems;Resumes;Schedules;Modelling Queueing networks;Software Performance;Virtual execution}, 
doi={10.1109/QEST.2012.26}, 
ISSN={}, 
month={Sept},}